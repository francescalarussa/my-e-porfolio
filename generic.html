<!DOCTYPE HTML>
<!--
	Forty by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Modules| Larussa Francesca</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo"><strong>Francesca</strong> <span>Larussa</span></a>
						<nav>
							<a href="#menu">Menu</a>
						</nav>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<ul class="links">
							<li><a href="index.html">Home</a></li>
							<li><a href="landing.html">About me</a></li>
							<li><a href="generic.html">Modules</a></li>
							<li><a href="elements.html">Skills</a></li>
						</ul>
						<ul class="actions stacked">
							<li><a href="#" class="button primary fit">Get Started</a></li>
							<li><a href="#" class="button fit">Log In</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main" class="alt">

						<!-- One -->
							<section id="one">
								<div class="inner">
									<header class="major">
										<h1>Modules</h1>
									</header>
									<span class="image main"><img src="images/pic11.jpg" alt="" /></span>
									<p> This section brings together some of the work produced during the Units of the course "Deciphering Big Data"</p>
									<section class="box">
										<h2> Unit 1 - Introduction to Big Data Technologies and Data Management</h2>
										<p> During the first unit, I had the opportunity to explore the definition of big data and the methods and techniques for managing it, which were organized into four steps: cleansing, standardization, formatting, and normalization. I also reviewed the definitions of data formats, including structured, semi-structured, and unstructured data, and the definitions of CSV, JSON, and XML, which will be revisited in the following section. As an activity, I picked up the e-portfolio that I had already started and began organizing the main page and the “About Me” section. In addition, I participated in the collaborative discussion on “the data collection process.”</p>
										<h3>Collaborative Discussion - Initial Post</h3>
										<p> As mentioned above, during this Unit, I took part in a collaborative discussion on data collection process. The screenshot below represents my initial post/p>
										<span class="image fit">
											<img src="images/unit1.png" alt="Unit 1" style="max-width:100%; height:auto;">
										</span>
									</section>
								<section class="box">
										<h2> Unit 2 - Introduction to data Types and Formats</h2>
										<p> In this Unit, the focus was on tools to manage big data that are useful for managing, analyzing, and visualizing it. After reading Chapter 8 (8.29 - 8.35) of Sardar and Pandey's book titled "Big Data Computing”, I learned that typical analytical tools are used to manage, refine, discover, predict, and validate data. The most commonly used tools are NoSQL,Cassandra, Apache Hadoop and MapReduce, Apache Mahout, Storm, and Apache Spark. The choice of one tool over another is based on the specific needs. Considering the ones listed before, NoSQL is chosen for managing unstructured and non-relational data, Cassandra ensures replication and high availability, Hadoop and MapReduce can be chosen for distributed storage and batch processing, Mahout is used for scalable machine learning algorithms, Storm is used for real-time streaming data, and Spark allows fast in-memory processing. Delving into this aspect allowed me to participate in the team project meeting with in-deep knowledge of the topic and engage in the discussion on which server would be better for the implementation of the database. During this unit, I also continued my collaborative discussion on "The Data Collection Process," writing two peer reviews.</p>
										<h3> Collaborative Discussion - Peer Reviews</h3>
										<p> As mentioned above, during this Unit, I took part in a collaborative discussion on data collection process. The screenshot below represents my peer reviews</p>
										<span class="image fit">
											<img src="images/unit2peer.png" alt="Unit 1" style="max-width:100%; height:auto;">
											<img src="images/unit2peer2.png" alt="Unit 1" style="max-width:100%; height:auto;">
											</span>
									</section>
										<section class="box">
									    <h2> Unit 3 - Data Collection and Storage</h2>
										<p> During Unit 3, I learned to use the pandas library for data cleaning and preparation.  Because the datasets contained missing values, poorly formatted strings, duplicates, outliers, and repeated categories, I was able to overcome these issues using pandas.For example, Pandas allows us to recognize the missing values and remove or modify them and transform data through commands such as Replace, rename, map, cut, and qcut , which were used when working with categorical data. In addition, the use of categorical data commands, such as group by and value_counts, is faster and uses less memory.In the book “Big Data Computing: Advances in Technologies, Methodologies and Applications”,  I learned that big data can be managed in two ways: batch processing and stream processing. Batch processing can be used to elaborate on largevolumes of data periodically, whereas stream processing is used for real-time analysis. Cloud providers offer tools for storage, processing, analytics, and data orchestration. The major advantages are scalability and advanced insights, while the disadvantages are data security, cost, and complexity. During this Unit, I concluded the collaborative discussion “The Data Collection Process," with a summary post, and I also did a web scraping exercise and posted it on the wiki module.</p>
										<h3> Collaborative Discussion - Summary Post</h3>
										<p> As mentioned above, during this Unit, I took part in a collaborative discussion on data collection process. The screenshot below represents my summary post</p>
										<span class="image fit">
											<img src="images/Unit3Sum.png" alt="Unit 1" style="max-width:100%; height:auto;">
										<h4>Web Scrapping exercise</h4>
										<p>
                                        Web Scraping

                                        In this activity, I created a Python script to extract and organize information from the Prospectus webpage discussing the Data Scientist role. The goal was to identify the key content of the professional profile. The data were then collected in a JSON file to allow easy and structured consultation for the content analysis.

										The script is as follows:
											<pre><code>
											from bs4 import BeautifulSoup
										    import requests
											import json
                                            url= "https://www.prospects.ac.uk/job-profiles/data-scientist"
                                            response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
											soup= BeautifulSoup(response.text, "html.parser")
											print("Title:", soup.find("h1").text)
											print("\n Data Scientist content:\n")
											results = soup.find_all(string=lambda text: "data scientist" in text.lower())
											for r in results[:10]:
											print("-", r.strip())
											with open("data_science.json", "w") as f:
											json.dump({"results":[r.strip() for r in results]}, f, indent=4)
											print("\nFile JSON correctly created”)
											</code></pre>

                                        The results can be found in the attached file.

										After completing this activity, I can affirm that using libraries such as “Requests” and “BeautifulSoup” allows for collecting the desired data efficiently. In addition, Web Scraping enables the isolation of key information without the need to read the entire webpage. This methodology can be extremely useful in other contexts, allowing systematic and automated data collection.</p>
											<a href="assets/data_science.json" download>Download JSON</a>

										</span>
									</section>
									<section class="box">
										<h2> Unit 4 - Data Cleaning and Transformation </h2>
										<p> In Unit 4, the focus is on data cleaning. The main commands are: <br>
											
                                          - Isna(), notna(), dropna(), fillna() used to handle missing values<br>
                                          - Duplicated(), drop_duplicates() used to remove duplicate rows<br>
                                          - Map(), replace() used to transform and replace values<br>
                                          - Rename() used to rename rows and columns<br>
                                          - Cut() and qcut() used for binning and discretization<br>
                                          - Abs(), any() used to filter outliers<br>
                                          - Sample(), permutation () used to randomly select and shuffle data<br>
                                          - Get_dummies() is used to create indicator/dummy variables for machine learning<br>
                                          - .astype() with ‘Int64’, ‘string’, and ‘boolean’ Pandas extension types that allow missing values in integer, string, or boolean columns.<br>
During this Unit, there was an exercise to follow from the book “Data Wrangling with Python” by Kazil and Jarmul (2016). Although the pages were specified in the exercise, I could not find them because the book was available online. I searched for the terms “mn.csv” and “mn_header.csv,” and the search led me to Chapter 7. In addition, I completed the data management pipeline test, and the results are visible in the screenshot below.
Through this unit, I learned that the data cleaning process is extremely important and directly affects data quality and reliability for subsequent analysis or machine learning applications.</p>
										
										<h3>Lecturecast Exercise</h3>
										
										<p> As mentioned above, during this Unit, I completed the exercise proposed in the lecturecast. Above it is possible to download the PDF with the explanation of the exercise, and the python code. Here it is possible to see a screenshot of the results. </p>
										
										<span class="image fit">
											
											<img src="images/result_exercise_4.png" alt="Unit 4" style="max-width:100%; height:auto;">
											<img src="images/final_mn.png" alt="Unit 4" style="max-width:100%; height:auto;">
											<section class="download-section">
												
											<h4 class="page-title">Exercise download</h4>
											<ul>
												<li>
											
											<p>Here you can download the original dataset</p>
											<a href="assets/mn_sav" download>Download orginal dataset</a>
											</li>
												
											<li>
											<p>Here you can download the resulting dataset from the exercise</p>
											<a href="assets/mn_reduced.csv" download>Download post exercise dataset</a>
											</li>
											
											<li>
											<p>Here you can download the python code</p>
											<a href="assets/code_mn.py" download>Download Python code</a>
											</li>
											
											<li>
											<p>Here you can download the exercise explanation</p>
											<a href="assets/Exercise_Unit4.pdf" download>Download Exercise explanation</a>
											</li>
							</ul>
											<h5 class="page-title">  Data Management Pipeline Test</h5>
											
											<p>During the unit, I completed Data Management Pipeline Test and made three attempts. On the fist attempt, I scored 4.77, on the second attempt I scored 8.75 and finally on the last attempt, I scored 10.00 as shown in the screenshot</p>
											
											<span class="image fit">
											<img src="images/Pipeline_test.png" alt="Unit 4" style="max-width:100%; height:auto;">
											</span>
									</section>
									<section class="box">
										<h2> Unit 5- Data Cleaning and Automating Data Collection </h2>
										<p>This unit was the natural continuation of Unit 4 and focused on Big Data analytics and pipeline procedures.<br>
										Big data analytics enables us to analyse large quantities of data to find patterns and trends to improve decision-making. The main differences from traditional data analysis are that big data uses all types of data (structured, unstructured, and semi-structured), is faster because it processes data in real time, uses tools such as Hadoop, Spark, NoSQL, and machine learning, and can perform predictive and prescriptive analyses.<br>
										In big data analytics, the pipeline process is described as follows: <br>
										- Data collection through databases, sensors, etc<br>
										- Data storage using HDFS or a NoSQL database<br>
										- Data processing using Spark, Flink, and MapReduce <br>
										- Data analysis using machine learning, statistics, and prediction <br>
										- Data visualisation using a dashboard for faster decision-making <br>
										Conversational AI can be defined as a chatbot and virtual assistant that understands and communicates using human language.<br>
										The case study presented in this unit was the use of both BDA and Conversational AI in agriculture to support productivity, sustainability, cost reduction, and food security.<br>
										The concepts of Data Pipeline Automation and DataOps are discussed next. The data pipeline can be defined as the process that allows the collection, transformation, and preparation of data for machine learning or data analysis. DataOps aims to automate these practices to improve productivity and speed. The main challenge is that, nowadays, pipeline processes are usually not repeatable because they are managed by different people using different tools. This slows down development and increases the possibility of errors in the model.<br>
										An example of DataOps is Infoworks.io, which automates the data pipeline to handle tasks such as data ingestion, transformation, and preparation.<br>
										Through this unit, I delved into the pipeline process, and I learned that data pipelines can integrate multiple technologies to transform raw data into practical insights.</p>
										</span>
									</section>
									
										<section class="box">
                                        <h2> Unit 6 - Database design and Normalisation</h2>
										<p> During Unit 6, the main focus was on database design and normalisation. In particular, the unit highlights the importance of data cleaning, as well as the concept of standardisation, meaning transforming data to have a mean of 0 and a standard deviation of 1, and of normalisation, meaning scaling data values between 0 and 1. </p>
                                        The normalisation process was explored in depth, explaining how to prevent anomalies and introducing the Normal Forms, which are three:<br>
                                             -	1NF: all data has a single value <br>
                                             -	2NF: each data depends on a primary key, and must be in 1NF <br>
                                             -	3NF: no data depends on other data that are not key and must be in 2NF <br>
                                        Particularly important was the definition of primary keys, used to maintain integrity, and foreign keys, used to ensure referential integrity. This aspect was useful also in the perspective of the team project, which was to be submitted at the end of the unit.<br>
                                        In this unit, the main activity was to submit the team project and the peer evaluation template.<br>
                                        Below, it is possible to download the PDF of the team project and the peer evaluation template.</p>

										<h3>Development Team Project - Project Report</h3>
										<p> As mentioned above, during this Unit, I had to submit a Team project that can be downloaded here </p>
										<a href="assets/Team_project_Unit6.pdf" download>Download PDF</a>
										<h4>Peer Evaluation Template</h4>
										<p>Here you can download my Peer Evaluation Template</p>
						                <a href="assets/Peer_Evaluation_Template.pdf" download>Download PDF</a>
										</span>
									</section>
										
								<section class="box">
								 <h2> Unit 7 - Costructing Normalise Table and Database Build</h2>	
								<p>In Unit 7, the main learning outcomes focused on database design, implementation, and database management system performance comparisons.
The book on database design and implementation was extremely useful for learning more about the information system, SQL operations, normalization, database design and modelling, and how to transform data models into data designs.
The article on database management system performance comparisons defined the three major DBMS families: RDBMS, NoSQL, and NewSQL, highlighting the advantages and disadvantages of each system.
In this Unit, there were two activities: a normalization task and a data build task; both exercises are presented below.
Through this unit, I learned that different database management systems significantly impact performance, scalability, and use-case suitability. This insight was particularly important during the writing of my final project.
</p>
<h3> Normalisation and Database Build task</h3>
<p>During this unit, I completed two main practical tasks focused on database design and data integrity. <br>
- Normalisation task: normalise an un-normalised data table to the Third Normal Form (3NF) <br>
- Database build task: implementation of a relational database using MySQL</p>
<h4> Exercise download</h4>
<ul>
	<li>
		Here you can download the SQL code (Database Build Task)<br>
		<a href="assets/school.sql" download>Download sql code</a>
		</li>
	<li>
		Here you can download the normalisation task explanation <br>
		<a href="assets/Normalisation_task.pdf" download>Download PDF</a>
	</li>
	<li>
		Here you can download the database build task explanation <br>
		<a href="assets/Data_build_task.pdf" download>Download PDF</a>
	</li>
</ul>		
</span>
									</section>									
<section class="box">
<h2> Unit 8 - Compliance and Regulatory Framework for Managing Data</h2>
	<p>In Unit 8, the focus was on compliance and regulatory frameworks.  Compliance comprises standards, regulations, policies, and controls, all of which work in synergy with existing rules. The main role of compliance is to ensure that businesses consider the implications of using big data and that new data types and methodologies meet legislative requirements.<br>
	Compliance goals include controlling access through processes, securing data at rest, protecting and storing cryptographic keys, and creating trusted applications and environments to protect the data. The unit also addresses security challenges, such as the need to protect big data, the fact that the data are all unique and often impossible to recreate if lost, and the importance of controlling access to the data.  Additional issues discussed include availability, performance, and liability.<br>
	The unit then focuses on the UK GDPR, particularly Articles 5 and 6, and the ISO/IEC 27000 standard, which is the one adopted in the UK.<br>
	The ISO/IEC 27000 series focus on information security in businesses to prevent data breaches and cyberattacks.<br>
	In this unit, the main activity was participating in the collaborative discussion “Comparing Compliance Law” by making an initial post.<br>
	This unit allowed me to understand the importance of legal compliance and security standards in database design. It had a significant impact during the writing of my final project, as it enabled me to produce a more comprehensive and robust project.</p>
	<h3> Collaborative Discussion - Initial post</h3>
										<p> As mentioned above, during this Unit, I took part in a collaborative discussion on Comparing compliance laws. The screenshot below shows my initial post</p>
										<span class="image fit">
											<img src="images/Unit_8_Initial.png" alt="Unit 8" style="max-width:100%; height:auto;">
	</span>
									</section>
<section class="box">
	<h2>Unit 9 - Data Management Systems and Models</h2>
	<p>Unit 9 focuses on database models and their applications. There are three approaches to data management:<br>
	- Traditional: creates separate files for each application. The major challenges are data redundancy and data integrity.<br>
	- Database: A pool of related data shared among multiple applications. The main advantages are: control of data redundancy and flexibility.<br>
	- DBMS that is composed of three parts: hardware (from a standalone computer to a network of computers), software, which includes DBMS, operating system, and network applications.<br>
	- Data used by the organisation and schema description (description of data) <br>
	A database can be hierarchical, which is easy to manipulate and has easy relations, a network that is more flexible, or relational, which allows linking relational tables. <br>
	Before designing a database, the following must be considered: content, access, logical structure, which is a model of how the database is structured to meet requirements, and physical organization, which refers to how data will be stored and organized in the database.<br>
	The DBMS to be implemented must support SQL (a programming language for extracting information from databases). In addition, the DBMS must support multiple users and be sufficiently efficient to handle large datasets and support easy migration between hardware.<br>
	The core artifacts for database implementation are as follows:<br>
	- A schema, which contains information about the database. It contains user restriction information, security measures, and access control.<br>
	- Data dictionary: description of all the data in the database <br>
	It is also important to list some key terms:<br>
	- Relation: a table with columns and rows<br>
	- Attribute: named columns of a relation<br>
	- Domain: Set of allowable values<br>
	- Tuple: a record of a relation<br>
	- Base table: record physically stored in the database<br>
	- Primary key: unique tuple identifier<br>
	- Foreign key: a set of attributes with one relation that match a candidate key of another table<br>
	Moving on to the database view, it is possible to define a view as a virtual relation that can also not exist in the database and is created upon a request. The view content is a query on one or more base relations to simplify the complex operations. Customized data through views and queries can show different data to different users.<br>
	In a database, an application program can be defined as a series of transitions that act on the data for a particular function. The transaction can be either successful or unsuccessful. Concurrency control allows the management of simultaneous operations on the database without interference.<br>
	Moving on to big data architecture, it is possible to affirm that big data architectures manage the ingestion, processing, and analysis of data that are too big or complex for traditional systems. Big data architecture supports batch and real-time data processing, interactive exploration, predictive analysis, and ML.<br>
	An example of an architecture is the Internet of Things (IoT), an event-driven architecture for devices that generate large volumes of data. Devices send events through gateways, stream processing allows real-time analytics, and data is stored for batch analysis and machine learning.<br>
	During this unit, I continued the collaborative discussion on “comparing compliance law,” posting two peer reviews, and I tried to create a DBMS from Python for a seminar activity.</p>
	<h3> Collaborative Discussion - Peer reviews</h3>
	<p>As mentioned above, during this Unit, I took part in a collaborative discussion on Comparing compliance laws. The screenshot below shows my two peer reviews</p>
	<span class="image fit">
	<img src="images/Unit_8_peer_review.png" alt="Unit 8" style="max-width:100%; height:auto;">
	<span class="image fit">
	<img src="images/Unit_8_peer_review2.png" alt="Unit 8" style="max-width:100%; height:auto;">
	<h4>Seminar Activity - Database build</h4>
	<p>During the Unit, I completed an database-building with Python. Below, you can download the python code, the database file and the exercise explanation</p>
	<h4> Exercise download</h4>
<ul>
	<li>
		Here you can download the database file<br>
		<a href="neonatal_mortality_data.db" download>Download database file</a>
		</li>
	<li>
		Here you can download the exercise explanation <br>
		<a href="assets/DBMS_child_mortality.pdf" download>Download PDF</a>
	</li>
	<li>
		Here you can download python code <br>
		<a href="assets/child_mortality_python_code.py" download>Download python code</a>
	</span>
									</section>
<section class="box">
<h2>Unit 10 - More on APIs (Application Programming Interfaces) for Data Parsing</h2>
<p>In Unit 10, the focus was on Application Programming Interfaces and database architecture for the book “Data Engineering Best Practices” by Schiller and Larochelle (2024), from which I selected Chapters 6 and 7.<br>
The logical architecture is structured into three zones: bronze, silver, and gold.  Gold data represent a single source of truth and are strongly protected and semantically defined; silver data can be considered intermediaries because they support transformation, enrichment, and caching.<br>
Another component of the logical architecture is the data lake, which guarantees scalability, but it is not always useful in transactional cases; therefore, the architecture must also include high-performance caches, data marts, and in-memory systems. Data exchange must be managed through Change Data Capture, pub/sub systems, and code-based integrations.<br>
The consumption zone provides access to data through analytics, BI, OLAP, ML graphs, and security analytics.   Another extremely important component is operational management, which includes provisioning, monitoring, and observability. A cost estimator must be included, considering that every choice made during the logical phase affects costs.<br>
Another aspect that affects cost is the choice of cloud, on-premises, hybrid, or multi-cloud.<br>
In conclusion, the logical architecture includes components such as data quality, standardized flows, contracts, security, cost control, and scalability. Adopting an integrated vision of technology, business, and finance allows the creation of a sustainable platform that supports all business requirements.<br>
The physical architecture translates logical features into concrete components, configurations, processes, and roles. During this phase, the technologies are executed, integrated, operated, and monitored.<br>
These technologies are represented by physical diagrams that are readable as orchestrated flows and configurations that are managed through configuration management tools. In addition, it describes the failover process, recovery, scaling, and orchestration of the system.<br>
Another important aspect is environment management, including development, integration, UAT/preproduction, production, audit, cold/warm standby, and high availability.<br>
The physical architecture must be supported by ITTL processes, such as release, change, incident, and problem management, and by key roles, such as enterprise architect, chief engineer, and site reliability engineer.<br>
To realize a solid, scalable, and future-proof data factory, logical and physical architectures must be aligned.<br>
In this Unit, the IBM-QRADAR-Intellas-KAIF Integration is a case study.
KAIF is a platform that uses AI algorithms to analyse a large volume of security data and extract digital evidence from it. It focuses on two principles: verifying that the data are sent to the correct destination and ensuring that the information is secured even if it is intercepted<br>
  QRADAR is a security information and event management system that collects and manages network logs and events, performs forensic analysis, and rebuilds incidents, supporting both stand-alone and distributed deployments with high scalability and advanced threat analysis.<br>
KAIF is integrated with QRADAR through Inter-Process Communication using QRadar’s Device Support Model. The result is that the data are collected by QRadar and analyzed by KAIF, which, as a result, provides information, threat indicators, and intelligence to QRadar.<br>
The advantages are:<br>
- Strengthening QRadar capability through KAIF machine learning<br>
- Deep threat analysis<br>
- Output generation that is adaptable to the security needs<br>
During this Unit, the main activity was “API Security Requirements,” and continued the collaborative discussion “Comparing Compliance Laws” by posting a summary post.<br>
This Unit helped me gain a better understanding of the database building phases, including logical and physical architecture, which proved extremely useful during the writing of my final project.</p>
<h3 class="page-title"> Collaborative discussion - Summary Post </h3>
<p>During this Unit, I took part in a collaborative discussion on Comparing compliance laws. The screenshot below shows my Summary Post</p>
<span class="image fit">
	<img src="images/Unit_8_summary.png" alt="Unit 8" style="max-width:100%; height:auto;">
<h4 class="page-title">API Security Requirements</h4>
<p>As mentioned above, during this Unit, I completed the API Security Requirements exercise. Below you can download the exercise PDF</p>
<a href="assets/API_Requirements.pdf" download>Download PDF</a>	
section class="box">
<h2>Unit 11 - DBMS Transaction and Recovery</h2>
<p>During Unit 11, I had the opportunity to delve into uncommon file types and PDFs that are challenging to manage.<br>
The general strategy to be followed is as follows:<br>
- Identify a file type: control the file type or use the Python library python-magic<br>
- Browsing online solutions <br>
- Opening manually with a text editor or using the command open() in Python, if the characters are odd, verify the encoding <br>
- Convert the data into an easier format.<br>
The core Python tools for working with PDF files are as follows:<br>
- slate: read the PDF as a memory string<br>
- pdfminer: convert the PDF into text<br>
- pdftable: use pdfminer and try to rebuild the tables<br>
- tabula: extract the table in CSV<br>
In addition, I used the Trailhead website to delve into data presentation. The three best practice categories for designing a dashboard are planning, informative design, and finishing.<br>
Every element of the dashboard must be used to explain the data, and the use of colours or tooltips is used only to guide the interpretation. To create a good dashboard, clutter must be removed and significant differentiation added. In addition, it is necessary to add information that explains what the graph shows and tooltips with detailed information, while keeping them clear.<br>
To create a clear dashboard in Tableau by combining vizzes and controlling the layout, dimension, and presentation to guide the user in understanding the data, the following key points must be considered:<br>
- Define the dashboard dimension based on where it will be visualised<br>
- Add clear titles and explanatory texts<br>
- Remove elements that will distract the user<br>
- Organize the layout and rename the dashboard with a clear name.<br>
To create an interactive dashboard:<br>
- Add a filter to help navigate data<br>
- Show only relevant values in the filter<br>
- Test the filter<br>
n Unit 11, there are additional notes on database transactions and recovery.<br>
A transaction is a series of database operations that can be fully executed or not. The transactions follow the ACID properties, which are: <br>
- Atomicity: all is executed, or nothing is executed <br>
- Consistency: the database moves from one consistent status to another<br>
- Isolation: the transactions are isolated<br>
- Durability: Once completed, the modifications are permanent and irreversible<br>
The transaction manager manages the transaction and guarantees ACID properties. The use of commit produces permanent transactions, and the rollback cancels the modification if the transaction fails.<br>
The transaction log is a file that records all transactions and how to cancel them, and contains previous and new values, a timestamp, and the transaction state.<br>
If there is a system failure, to know how far back in the log file we must go, we can use checkpoints, which are system-controlled points at which synchronization occurs between the database and log file.<br>
To recover from a system failure, all transactions that were running during the failure must be undone and restarted. A serious issue arises when there is a media failure; to recover from it, a backup is needed. Backups must be performed frequently to minimize data loss.<br>
In this unit, I submitted a part of my final exam and posted on the wiki module an evaluation of the GFS backup procedure.<br>
This Unit helped me delve into new concepts, such as transactions and backups, which are crucial aspects of database management. It allowed me to understand the importance of anticipating potential failures and errors when designing a database.</p>
<h3 class="page-title"> Back Up Procedure </h3>
<p>During this Unit, I posted a text on GFS backup procedure on wiki module as it is possible to see in the screenshot below</p>
<span class="image fit">
	<img src="images/Backup_wiki_post.png" alt="Unit 11" style="max-width:100%; height:auto;">

			
								</div>
							</section>

					</div>

				<!-- Contact -->
					<section id="contact">
						<div class="inner">
							<section>
								<form method="post" action="#">
									<div class="fields">
										<div class="field half">
											<label for="name">Name</label>
											<input type="text" name="name" id="name" />
										</div>
										<div class="field half">
											<label for="email">Email</label>
											<input type="text" name="email" id="email" />
										</div>
										<div class="field">
											<label for="message">Message</label>
											<textarea name="message" id="message" rows="6"></textarea>
										</div>
									</div>
									<ul class="actions">
										<li><input type="submit" value="Send Message" class="primary" /></li>
										<li><input type="reset" value="Clear" /></li>
									</ul>
								</form>
							</section>
							<section class="split">
								<section>
									<div class="contact-method">
										<span class="icon solid alt fa-envelope"></span>
										<h3>Email</h3>
										<a href="#">larussafrancesca1@gmail.com</a>
									</div>
								</section>
								<section>
									<div class="contact-method">
										<span class="icon solid alt fa-phone"></span>
										<h3>Phone</h3>
										<span>(+39) 3664182661</span>
									</div>
								</section>
								<section>
									<div class="contact-method">
										<span class="icon solid alt fa-home"></span>
										<h3>Address</h3>
										<span> 
									    Turin (TO)<br />
										Italy</span>
									</div>
								</section>
							</section>
						</div>
					</section>

				<!-- Footer -->
					<footer id="footer">
						<div class="inner">
							<ul class="icons">
								<li><a href="#" class="icon brands alt fa-twitter"><span class="label">Twitter</span></a></li>
								<li><a href="#" class="icon brands alt fa-facebook-f"><span class="label">Facebook</span></a></li>
								<li><a href="#" class="icon brands alt fa-instagram"><span class="label">Instagram</span></a></li>
								<li><a href="#" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
								<li><a href="#" class="icon brands alt fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
							</ul>
							<ul class="copyright">
								<li>&copy; Untitled</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>

</html>



































<!DOCTYPE HTML>
<!--
	Forty by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Modules| Larussa Francesca</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo"><strong>Francesca</strong> <span>Larussa</span></a>
						<nav>
							<a href="#menu">Menu</a>
						</nav>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<ul class="links">
							<li><a href="index.html">Home</a></li>
							<li><a href="landing.html">About me</a></li>
							<li><a href="generic.html">Modules</a></li>
							<li><a href="elements.html">Skills</a></li>
						</ul>
						<ul class="actions stacked">
							<li><a href="#" class="button primary fit">Get Started</a></li>
							<li><a href="#" class="button fit">Log In</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main" class="alt">

						<!-- One -->
							<section id="one">
								<div class="inner">
									<header class="major">
										<h1>Modules</h1>
									</header>
									<span class="image main"><img src="images/pic11.jpg" alt="" /></span>
									<p> This section brings together some of the work produced during the Units of the course "Deciphering Big Data"</p>
									<section class="box">
										<h2> Unit 1 - Introduction to Big Data Technologies and Data Management</h2>
										<p> During the first unit, I had the opportunity to explore the definition of big data and the methods and techniques for managing it, which were organized into four steps: cleansing, standardization, formatting, and normalization. I also reviewed the definitions of data formats, including structured, semi-structured, and unstructured data, and the definitions of CSV, JSON, and XML, which will be revisited in the following section. As an activity, I picked up the e-portfolio that I had already started and began organizing the main page and the “About Me” section. In addition, I participated in the collaborative discussion on “the data collection process.”</p>
										<h3>Collaborative Discussion - Initial Post</h3>
										<p> As mentioned above, during this Unit, I took part in a collaborative discussion on data collection process. The screenshot below represents my initial post/p>
										<span class="image fit">
											<img src="images/unit1.png" alt="Unit 1" style="max-width:100%; height:auto;">
										</span>
									</section>
								<section class="box">
										<h2> Unit 2 - Introduction to data Types and Formats</h2>
										<p> In this Unit, the focus was on tools to manage big data that are useful for managing, analyzing, and visualizing it. After reading Chapter 8 (8.29 - 8.35) of Sardar and Pandey's book titled "Big Data Computing”, I learned that typical analytical tools are used to manage, refine, discover, predict, and validate data. The most commonly used tools are NoSQL,Cassandra, Apache Hadoop and MapReduce, Apache Mahout, Storm, and Apache Spark. The choice of one tool over another is based on the specific needs. Considering the ones listed before, NoSQL is chosen for managing unstructured and non-relational data, Cassandra ensures replication and high availability, Hadoop and MapReduce can be chosen for distributed storage and batch processing, Mahout is used for scalable machine learning algorithms, Storm is used for real-time streaming data, and Spark allows fast in-memory processing. Delving into this aspect allowed me to participate in the team project meeting with in-deep knowledge of the topic and engage in the discussion on which server would be better for the implementation of the database. During this unit, I also continued my collaborative discussion on "The Data Collection Process," writing two peer reviews.</p>
										<h3> Collaborative Discussion - Peer Reviews</h3>
										<p> As mentioned above, during this Unit, I took part in a collaborative discussion on data collection process. The screenshot below represents my peer reviews</p>
										<span class="image fit">
											<img src="images/unit2peer.png" alt="Unit 1" style="max-width:100%; height:auto;">
											<img src="images/unit2peer2.png" alt="Unit 1" style="max-width:100%; height:auto;">
											</span>
									</section>
										<section class="box">
									    <h2> Unit 3 - Data Collection and Storage</h2>
										<p> During Unit 3, I learned to use the pandas library for data cleaning and preparation.  Because the datasets contained missing values, poorly formatted strings, duplicates, outliers, and repeated categories, I was able to overcome these issues using pandas.For example, Pandas allows us to recognize the missing values and remove or modify them and transform data through commands such as Replace, rename, map, cut, and qcut , which were used when working with categorical data. In addition, the use of categorical data commands, such as group by and value_counts, is faster and uses less memory.In the book “Big Data Computing: Advances in Technologies, Methodologies and Applications”,  I learned that big data can be managed in two ways: batch processing and stream processing. Batch processing can be used to elaborate on largevolumes of data periodically, whereas stream processing is used for real-time analysis. Cloud providers offer tools for storage, processing, analytics, and data orchestration. The major advantages are scalability and advanced insights, while the disadvantages are data security, cost, and complexity. During this Unit, I concluded the collaborative discussion “The Data Collection Process," with a summary post, and I also did a web scraping exercise and posted it on the wiki module.</p>
										<h3> Collaborative Discussion - Summary Post</h3>
										<p> As mentioned above, during this Unit, I took part in a collaborative discussion on data collection process. The screenshot below represents my summary post</p>
										<span class="image fit">
											<img src="images/Unit3Sum.png" alt="Unit 1" style="max-width:100%; height:auto;">
										<h4>Web Scrapping exercise</h4>
										<p>
                                        Web Scraping

                                        In this activity, I created a Python script to extract and organize information from the Prospectus webpage discussing the Data Scientist role. The goal was to identify the key content of the professional profile. The data were then collected in a JSON file to allow easy and structured consultation for the content analysis.

										The script is as follows:
											<pre><code>
											from bs4 import BeautifulSoup
										    import requests
											import json
                                            url= "https://www.prospects.ac.uk/job-profiles/data-scientist"
                                            response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
											soup= BeautifulSoup(response.text, "html.parser")
											print("Title:", soup.find("h1").text)
											print("\n Data Scientist content:\n")
											results = soup.find_all(string=lambda text: "data scientist" in text.lower())
											for r in results[:10]:
											print("-", r.strip())
											with open("data_science.json", "w") as f:
											json.dump({"results":[r.strip() for r in results]}, f, indent=4)
											print("\nFile JSON correctly created”)
											</code></pre>

                                        The results can be found in the attached file.

										After completing this activity, I can affirm that using libraries such as “Requests” and “BeautifulSoup” allows for collecting the desired data efficiently. In addition, Web Scraping enables the isolation of key information without the need to read the entire webpage. This methodology can be extremely useful in other contexts, allowing systematic and automated data collection.</p>
											<a href="assets/data_science.json" download>Download JSON</a>

										</span>
									</section>
									<section class="box">
										<h2> Unit 4 - Data Cleaning and Transformation </h2>
										<p> In Unit 4, the focus is on data cleaning. The main commands are: <br>
											
                                          - Isna(), notna(), dropna(), fillna() used to handle missing values<br>
                                          - Duplicated(), drop_duplicates() used to remove duplicate rows<br>
                                          - Map(), replace() used to transform and replace values<br>
                                          - Rename() used to rename rows and columns<br>
                                          - Cut() and qcut() used for binning and discretization<br>
                                          - Abs(), any() used to filter outliers<br>
                                          - Sample(), permutation () used to randomly select and shuffle data<br>
                                          - Get_dummies() is used to create indicator/dummy variables for machine learning<br>
                                          - .astype() with ‘Int64’, ‘string’, and ‘boolean’ Pandas extension types that allow missing values in integer, string, or boolean columns.<br>
During this Unit, there was an exercise to follow from the book “Data Wrangling with Python” by Kazil and Jarmul (2016). Although the pages were specified in the exercise, I could not find them because the book was available online. I searched for the terms “mn.csv” and “mn_header.csv,” and the search led me to Chapter 7. In addition, I completed the data management pipeline test, and the results are visible in the screenshot below.
Through this unit, I learned that the data cleaning process is extremely important and directly affects data quality and reliability for subsequent analysis or machine learning applications.</p>
										
										<h3>Lecturecast Exercise</h3>
										
										<p> As mentioned above, during this Unit, I completed the exercise proposed in the lecturecast. Above it is possible to download the PDF with the explanation of the exercise, and the python code. Here it is possible to see a screenshot of the results. </p>
										
										<span class="image fit">
											
											<img src="images/result_exercise_4.png" alt="Unit 4" style="max-width:100%; height:auto;">
											<img src="images/final_mn.png" alt="Unit 4" style="max-width:100%; height:auto;">
											<section class="download-section">
												
											<h4 class="page-title">Exercise download</h4>
											<ul>
												<li>
											
											<p>Here you can download the original dataset</p>
											<a href="assets/mn_sav" download>Download orginal dataset</a>
											</li>
												
											<li>
											<p>Here you can download the resulting dataset from the exercise</p>
											<a href="assets/mn_reduced.csv" download>Download post exercise dataset</a>
											</li>
											
											<li>
											<p>Here you can download the python code</p>
											<a href="assets/code_mn.py" download>Download Python code</a>
											</li>
											
											<li>
											<p>Here you can download the exercise explanation</p>
											<a href="assets/Exercise_Unit4.pdf" download>Download Exercise explanation</a>
											</li>
							</ul>
											<h5 class="page-title">  Data Management Pipeline Test</h5>
											
											<p>During the unit, I completed Data Management Pipeline Test and made three attempts. On the fist attempt, I scored 4.77, on the second attempt I scored 8.75 and finally on the last attempt, I scored 10.00 as shown in the screenshot</p>
											
											<span class="image fit">
											<img src="images/Pipeline_test.png" alt="Unit 4" style="max-width:100%; height:auto;">
											</span>
									</section>
									<section class="box">
										<h2> Unit 5- Data Cleaning and Automating Data Collection </h2>
										<p>This unit was the natural continuation of Unit 4 and focused on Big Data analytics and pipeline procedures.<br>
										Big data analytics enables us to analyse large quantities of data to find patterns and trends to improve decision-making. The main differences from traditional data analysis are that big data uses all types of data (structured, unstructured, and semi-structured), is faster because it processes data in real time, uses tools such as Hadoop, Spark, NoSQL, and machine learning, and can perform predictive and prescriptive analyses.<br>
										In big data analytics, the pipeline process is described as follows: <br>
										- Data collection through databases, sensors, etc<br>
										- Data storage using HDFS or a NoSQL database<br>
										- Data processing using Spark, Flink, and MapReduce <br>
										- Data analysis using machine learning, statistics, and prediction <br>
										- Data visualisation using a dashboard for faster decision-making <br>
										Conversational AI can be defined as a chatbot and virtual assistant that understands and communicates using human language.<br>
										The case study presented in this unit was the use of both BDA and Conversational AI in agriculture to support productivity, sustainability, cost reduction, and food security.<br>
										The concepts of Data Pipeline Automation and DataOps are discussed next. The data pipeline can be defined as the process that allows the collection, transformation, and preparation of data for machine learning or data analysis. DataOps aims to automate these practices to improve productivity and speed. The main challenge is that, nowadays, pipeline processes are usually not repeatable because they are managed by different people using different tools. This slows down development and increases the possibility of errors in the model.<br>
										An example of DataOps is Infoworks.io, which automates the data pipeline to handle tasks such as data ingestion, transformation, and preparation.<br>
										Through this unit, I delved into the pipeline process, and I learned that data pipelines can integrate multiple technologies to transform raw data into practical insights.</p>
										</span>
									</section>
									
										<section class="box">
                                        <h2> Unit 6 - Database design and Normalisation</h2>
										<p> During Unit 6, the main focus was on database design and normalisation. In particular, the unit highlights the importance of data cleaning, as well as the concept of standardisation, meaning transforming data to have a mean of 0 and a standard deviation of 1, and of normalisation, meaning scaling data values between 0 and 1. </p>
                                        The normalisation process was explored in depth, explaining how to prevent anomalies and introducing the Normal Forms, which are three:<br>
                                             -	1NF: all data has a single value <br>
                                             -	2NF: each data depends on a primary key, and must be in 1NF <br>
                                             -	3NF: no data depends on other data that are not key and must be in 2NF <br>
                                        Particularly important was the definition of primary keys, used to maintain integrity, and foreign keys, used to ensure referential integrity. This aspect was useful also in the perspective of the team project, which was to be submitted at the end of the unit.<br>
                                        In this unit, the main activity was to submit the team project and the peer evaluation template.<br>
                                        Below, it is possible to download the PDF of the team project and the peer evaluation template.</p>

										<h3>Development Team Project - Project Report</h3>
										<p> As mentioned above, during this Unit, I had to submit a Team project that can be downloaded here </p>
										<a href="assets/Team_project_Unit6.pdf" download>Download PDF</a>
										<h4>Peer Evaluation Template</h4>
										<p>Here you can download my Peer Evaluation Template</p>
						                <a href="assets/Peer_Evaluation_Template.pdf" download>Download PDF</a>
										</span>
									</section>
										
								<section class="box">
								 <h2> Unit 7 - Costructing Normalise Table and Database Build</h2>	
								<p>In Unit 7, the main learning outcomes focused on database design, implementation, and database management system performance comparisons.
The book on database design and implementation was extremely useful for learning more about the information system, SQL operations, normalization, database design and modelling, and how to transform data models into data designs.
The article on database management system performance comparisons defined the three major DBMS families: RDBMS, NoSQL, and NewSQL, highlighting the advantages and disadvantages of each system.
In this Unit, there were two activities: a normalization task and a data build task; both exercises are presented below.
Through this unit, I learned that different database management systems significantly impact performance, scalability, and use-case suitability. This insight was particularly important during the writing of my final project.
</p>
<h3> Normalisation and Database Build task</h3>
<p>During this unit, I completed two main practical tasks focused on database design and data integrity. <br>
- Normalisation task: normalise an un-normalised data table to the Third Normal Form (3NF) <br>
- Database build task: implementation of a relational database using MySQL</p>
<h4> Exercise download</h4>
<ul>
	<li>
		Here you can download the SQL code (Database Build Task)<br>
		<a href="assets/school.sql" download>Download sql code</a>
		</li>
	<li>
		Here you can download the normalisation task explanation <br>
		<a href="assets/Normalisation_task.pdf" download>Download PDF</a>
	</li>
	<li>
		Here you can download the database build task explanation <br>
		<a href="assets/Data_build_task.pdf" download>Download PDF</a>
	</li>
</ul>									
<section class="box">
<h2> Unit 8 - Compliance and Regulatory Framework for Managing Data</h2>
	<p>In Unit 8, the focus was on compliance and regulatory frameworks.  Compliance comprises standards, regulations, policies, and controls, all of which work in synergy with existing rules. The main role of compliance is to ensure that businesses consider the implications of using big data and that new data types and methodologies meet legislative requirements.<br>
	Compliance goals include controlling access through processes, securing data at rest, protecting and storing cryptographic keys, and creating trusted applications and environments to protect the data. The unit also addresses security challenges, such as the need to protect big data, the fact that the data are all unique and often impossible to recreate if lost, and the importance of controlling access to the data.  Additional issues discussed include availability, performance, and liability.<br>
	The unit then focuses on the UK GDPR, particularly Articles 5 and 6, and the ISO/IEC 27000 standard, which is the one adopted in the UK.<br>
	The ISO/IEC 27000 series focus on information security in businesses to prevent data breaches and cyberattacks.<br>
	In this unit, the main activity was participating in the collaborative discussion “Comparing Compliance Law” by making an initial post.<br>
	This unit allowed me to understand the importance of legal compliance and security standards in database design. It had a significant impact during the writing of my final project, as it enabled me to produce a more comprehensive and robust project.</p>
	<h3> Collaborative Discussion - Initial post</h3>
										<p> As mentioned above, during this Unit, I took part in a collaborative discussion on Comparing compliance laws. The screenshot below shows my initial post</p>
										<span class="image fit">
											<img src="images/Unit_8_Initial.png" alt="Unit 8" style="max-width:100%; height:auto;">
	

						
				
								</div>
							</section>

					</div>

				<!-- Contact -->
					<section id="contact">
						<div class="inner">
							<section>
								<form method="post" action="#">
									<div class="fields">
										<div class="field half">
											<label for="name">Name</label>
											<input type="text" name="name" id="name" />
										</div>
										<div class="field half">
											<label for="email">Email</label>
											<input type="text" name="email" id="email" />
										</div>
										<div class="field">
											<label for="message">Message</label>
											<textarea name="message" id="message" rows="6"></textarea>
										</div>
									</div>
									<ul class="actions">
										<li><input type="submit" value="Send Message" class="primary" /></li>
										<li><input type="reset" value="Clear" /></li>
									</ul>
								</form>
							</section>
							<section class="split">
								<section>
									<div class="contact-method">
										<span class="icon solid alt fa-envelope"></span>
										<h3>Email</h3>
										<a href="#">larussafrancesca1@gmail.com</a>
									</div>
								</section>
								<section>
									<div class="contact-method">
										<span class="icon solid alt fa-phone"></span>
										<h3>Phone</h3>
										<span>(+39) 3664182661</span>
									</div>
								</section>
								<section>
									<div class="contact-method">
										<span class="icon solid alt fa-home"></span>
										<h3>Address</h3>
										<span> 
									    Turin (TO)<br />
										Italy</span>
									</div>
								</section>
							</section>
						</div>
					</section>

				<!-- Footer -->
					<footer id="footer">
						<div class="inner">
							<ul class="icons">
								<li><a href="#" class="icon brands alt fa-twitter"><span class="label">Twitter</span></a></li>
								<li><a href="#" class="icon brands alt fa-facebook-f"><span class="label">Facebook</span></a></li>
								<li><a href="#" class="icon brands alt fa-instagram"><span class="label">Instagram</span></a></li>
								<li><a href="#" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
								<li><a href="#" class="icon brands alt fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
							</ul>
							<ul class="copyright">
								<li>&copy; Untitled</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>

</html>






























